{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17556a1e93d56324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Installing Packages and Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:35.568520200Z",
     "start_time": "2023-09-06T05:57:30.148448600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amuly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amuly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amuly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db320da2d517b9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:35.710745700Z",
     "start_time": "2023-09-06T05:57:35.566745200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  class                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "data = pd.read_csv(\"../data/data_cleaned.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d02a7747e6ab5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Transformation\n",
    "\n",
    "This section contains the steps to transforming so that it will be easier to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e154a7c4d31b77c2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:41.288618400Z",
     "start_time": "2023-09-06T05:57:35.651468200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Remove Punctuation Marks and URLs\n",
    "def remove_punctuation_and_urls(text):\n",
    "    # Remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(remove_punctuation_and_urls)\n",
    "\n",
    "# Step 2: Remove Stop Words and Lowercase\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords_and_lowercase(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and lowercase the tokens\n",
    "    filtered_tokens = [word.lower()\n",
    "                       for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords_and_lowercase)\n",
    "\n",
    "# Step 3: Tokenization, Stemming, and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tokenize_stem_lemmatize(text):\n",
    "    # Tokenize the cleaned text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Apply stemming and lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(tokenize_stem_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a891450dc649518a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:41.315176700Z",
     "start_time": "2023-09-06T05:57:41.293714300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  class                                               text\n0   ham  go jurong point crazi avail bugi n great world...\n1   ham                              ok lar joke wif u oni\n2  spam  free entri 2 wkli comp win fa cup final tkt 21...\n3   ham                u dun say earli hor u c alreadi say\n4   ham          nah dont think goe usf live around though",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>go jurong point crazi avail bugi n great world...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>ok lar joke wif u oni</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>u dun say earli hor u c alreadi say</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>nah dont think goe usf live around though</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea4d62d99ac40fe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:41.341056400Z",
     "start_time": "2023-09-06T05:57:41.304980500Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.apply(self._preprocess_text)\n",
    "        return X_transformed\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # Remove punctuation marks and URLs\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and lowercase the tokens\n",
    "        filtered_tokens = [word.lower()\n",
    "                           for word in tokens if word.lower() not in self.stop_words]\n",
    "\n",
    "        # Apply stemming and lemmatization\n",
    "        stemmed_tokens = [self.stemmer.stem(word) for word in filtered_tokens]\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(\n",
    "            word) for word in stemmed_tokens]\n",
    "\n",
    "        return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4faf838163a74da4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:43.402238100Z",
     "start_time": "2023-09-06T05:57:41.320684100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit and transform the 'text' column using the pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor())\n",
    "])\n",
    "\n",
    "# Fit and transform the 'text' column using the pipeline\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719e71a4e067fe60",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T05:57:43.497834600Z",
     "start_time": "2023-09-06T05:57:43.405532600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['ham', 'spam'], dtype=object)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the labels\n",
    "enc = LabelEncoder()\n",
    "data[\"class\"] = enc.fit_transform(data[\"class\"])\n",
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7debd16b8b856f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.162437700Z",
     "start_time": "2023-09-03T13:08:45.146656400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"text\"] = X_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8851beeca78d181c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.178083900Z",
     "start_time": "2023-09-03T13:08:45.162437700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting the data into features and labels \n",
    "features = data[\"text\"]\n",
    "labels = data[\"class\"]\n",
    "\n",
    "# splitting the data into train and test sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(features, labels, test_size=0.2,\n",
    "                                                                              random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8035ff706f4a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477067434b6756c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1cb52f0dcd7ee2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.225267Z",
     "start_time": "2023-09-03T13:08:45.178083900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_vocab_length = 10000  # max number of words to have in our vocabulary\n",
    "max_length = 15  # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=max_vocab_length,\n",
    "                                           output_mode=\"int\",\n",
    "                                           output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87582dd262957b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.546890800Z",
     "start_time": "2023-09-03T13:08:45.225267Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer on train sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3f2addd7026d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.611003100Z",
     "start_time": "2023-09-03T13:08:45.546890800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[   1,    1, 5383,    1,    1,  946,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd8051baf11c84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fe8f118aea790d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.611003100Z",
     "start_time": "2023-09-03T13:08:45.582481Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x2172d82fee0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,  # set input shape\n",
    "                             output_dim=128,  # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "                             input_length=max_length,  # how long is each input\n",
    "                             name=\"embedding_1\")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b954c3fea7fc56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9c0881d3164e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.683807700Z",
     "start_time": "2023-09-03T13:08:45.595484100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs)  # turn the input text into numbers\n",
    "x = embedding(x)  # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(\n",
    "    x)  # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(\n",
    "    x)  # create the output layer, want binary outputs so use sigmoid activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")  # construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37cebcb964d04e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:45.714408400Z",
     "start_time": "2023-09-03T13:08:45.683807700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9638b149bd92a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:09:37.763874600Z",
     "start_time": "2023-09-03T13:09:37.701385500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 15)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf02e0cc7850809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:08:58.378247500Z",
     "start_time": "2023-09-03T13:08:45.714408400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "130/130 [==============================] - 2s 9ms/step - loss: 0.4175 - accuracy: 0.9127\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 1s 8ms/step - loss: 0.1502 - accuracy: 0.9608\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0950 - accuracy: 0.9741\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0696 - accuracy: 0.9809\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0532 - accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0417 - accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0335 - accuracy: 0.9920\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9927\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 1s 11ms/step - loss: 0.0225 - accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 1s 9ms/step - loss: 0.0186 - accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2172d823640>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1.fit(train_sentences,\n",
    "            train_labels,\n",
    "            epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bdcd8bd94ec79f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T13:09:43.160558700Z",
     "start_time": "2023-09-03T13:09:42.931194400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 995us/step - loss: 0.0619 - accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06188231706619263, 0.9758220314979553]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on the test data\n",
    "model_1.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff78e41a037206",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "97% accuracy is pretty good so let's convert the above code into modular programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
